{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e2d958",
   "metadata": {},
   "source": [
    "# Needed Packages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26283d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install uv\n",
    "# %uv pip install --upgrade langchain langchain-community langchain-chroma\n",
    "# %uv pip install -qU langchain-groq\n",
    "# %uv pip install langchain_openai\n",
    "# %uv pip install --upgrade langchain_huggingface\n",
    "# %uv pip install --upgrade unstructured openpyxl\n",
    "# %uv pip install nltk\n",
    "# %uv pip install --upgrade --quiet langchain sentence_transformers\n",
    "# %uv pip install xlrd\n",
    "# %uv pip install xformers\n",
    "# %uv pip install pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45e363",
   "metadata": {},
   "source": [
    "* NOTE:\n",
    "    The `embedding model (Jina Embeddings V3)` and the `LLM (Llama 3.2 90b)` are used through API services offered by Jina AI and Groq respictively.\n",
    "    However, both are open-source and can be downloaded and used locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47910163",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c99dfa",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71358248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import nltk\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import glob\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_core.messages import trim_messages, AIMessage, HumanMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "## Imports for the PDF to text conversion using surya-ocr\n",
    "from PIL import Image\n",
    "# from surya.ocr import run_ocr\n",
    "# from surya.model.detection.model import load_model as load_det_model, load_processor as load_det_processor\n",
    "# from surya.model.recognition.model import load_model as load_rec_model\n",
    "# from surya.model.recognition.processor import load_processor as load_rec_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39f6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install uv\n",
    "\n",
    "# # Core LangChain\n",
    "# ! uv pip install langchain\n",
    "\n",
    "# # For Ollama integration (embeddings, models)\n",
    "# ! uv pip install langchain-ollama\n",
    "\n",
    "# # For Chroma vector database\n",
    "# ! uv pip install langchain-chroma chromadb\n",
    "\n",
    "# # For community loaders (Arxiv, PDF, text, etc.)\n",
    "# ! uv pip install langchain-community\n",
    "\n",
    "# # For text splitting utilities\n",
    "# ! uv pip install langchain-text-splitters\n",
    "\n",
    "# # Core types, prompts, and documents (usually installed with langchain-core)\n",
    "# ! uv pip install langchain-core\n",
    "\n",
    "# # Typing extensions (for TypedDict, List, etc.)\n",
    "# ! uv pip install typing-extensions\n",
    "\n",
    "# # LangGraph (for StateGraph)@\n",
    "# ! uv pip install langgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb133c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import ArxivLoader, TextLoader\n",
    "from langchain_community.document_loaders.pdf import BasePDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing_extensions import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langgraph.graph import StateGraph, START\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c954df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = init_chat_model(model=\"gemma3:1b\", model_provider=\"ollama\")\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e36009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the SQLite database for LLM caching\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# # API keys\n",
    "GROQ_API_KEY = os.environ['GROQ_API_KEY']\n",
    "# JINA_API_KEY = os.environ['JINA_API_KEY']\n",
    "\n",
    "# Needed downloads for nltk (Only needs to be done once)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2a48e",
   "metadata": {},
   "source": [
    "# Data Indexing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd0c17",
   "metadata": {},
   "source": [
    "## 1- Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94142b3d",
   "metadata": {},
   "source": [
    "### For Excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24976c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_files(file_paths:list) -> list:\n",
    "    \"\"\"\n",
    "    Load Excel files and return a list of Langchain Documents.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list): List of file paths to Excel files\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Langchain Documents\n",
    "    \"\"\"\n",
    "    loader = UnstructuredExcelLoader(file_paths, mode='elements')\n",
    "    doc = loader.load()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa5460",
   "metadata": {},
   "source": [
    "### For txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6845da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_files(file_path:str):\n",
    "    \"\"\"\n",
    "    Loads a text file and returns a Langchain Document.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the text file\n",
    "\n",
    "    Returns:\n",
    "        Document: A Langchain Document\n",
    "    \"\"\"\n",
    "    loader = TextLoader(file_path=file_path, encoding='utf-8')\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8999e13",
   "metadata": {},
   "source": [
    "## 2-Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f871304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the doc into smaller chunks i.e. chunk_size=512\n",
    "def split_documents(docs: list, chunk_size=512, chunk_overlap=128) -> list:\n",
    "    \"\"\"\n",
    "    Splits the provided documents into smaller chunks with specified size and overlap.\n",
    "\n",
    "    Parameters:\n",
    "        docs (list): List of documents to be split.\n",
    "        chunk_size (int, optional): The number of characters in each chunk (default is 512).\n",
    "        chunk_overlap (int, optional): The number of overlapping characters between chunks (default is 128).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of split document chunks with corrected metadata.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Fixing the metadata if something is wrong with it\n",
    "    for chunk in chunks:\n",
    "        for key, value in chunk.metadata.items():\n",
    "            if isinstance(value, list):\n",
    "                chunk.metadata[key] = ','.join(value)  # Convert list to a comma-separated string\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c6be3",
   "metadata": {},
   "source": [
    "## 3-Data Embedding and data storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2108899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(chunks: list, CHROMA_PATH=\"vec_db\"):\n",
    "    \"\"\"\n",
    "    Store the given chunks as embeddings in a Chroma database.\n",
    "\n",
    "    Parameters:\n",
    "        chunks (list of Document): list of documents to be embedded\n",
    "        CHROMA_PATH (str, optional): path where the Chroma database is stored (default is \"vec_db\")\n",
    "\n",
    "    Returns:\n",
    "        Chroma: the created Chroma database\n",
    "    \"\"\"\n",
    "    store = LocalFileStore(\"./emb_cache/\")\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "    cached_embedder = CacheBackedEmbeddings.from_bytes_store(embeddings, store, query_embedding_cache=True, key_encoder=lambda x: hashlib.sha256(x.encode()).hexdigest())\n",
    "    \n",
    "    # embed the chunks as vectors and load them into the database\n",
    "    db_chroma = Chroma.from_documents(chunks, cached_embedder, persist_directory=CHROMA_PATH)\n",
    "    return db_chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22fdcb",
   "metadata": {},
   "source": [
    "# Data Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efb9fe",
   "metadata": {},
   "source": [
    "## 1-Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff98df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(db_chroma, chunks, query: str, k=50) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Retrieve context - top k most relevant (closest) chunks to the query vector using an ensemble retriever.\n",
    "    \n",
    "    Parameters:\n",
    "        db_chroma (Chroma): database of embeddings\n",
    "        chunks (list): List of document chunks.\n",
    "        query (str or np.ndarray): user query as a string or a vector\n",
    "        k (int, optional): number of documents to retrieve (default is 50)\n",
    "        \n",
    "    Returns:\n",
    "        list of tuple: list of retrieved documents and their scores\n",
    "    \"\"\"\n",
    "    # Initialize the BM25 retriever\n",
    "    bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "    bm25_retriever.k = k\n",
    "\n",
    "    # Initialize the Chroma retriever\n",
    "    chroma_retriever = db_chroma.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    # Initialize the Ensemble Retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, chroma_retriever], weights=[0.6, 0.4]\n",
    "    )\n",
    "    \n",
    "    docs = ensemble_retriever.invoke(query)\n",
    "    # The ensemble retriever in langchain doesn't support returning scores, so we return a list of documents\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b8b5d",
   "metadata": {},
   "source": [
    "## 2-Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d164a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(docs_chroma: list, query: str, past_messages:str) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer based on given user query and retrieved context information\n",
    "    \n",
    "    Parameters:\n",
    "        docs_chroma (list of Document): retrieved context information\n",
    "        query (str): user query\n",
    "        past_messages (str): past questions and answers\n",
    "    Returns:\n",
    "        str: answer to the user query\n",
    "    \"\"\"\n",
    "\n",
    "    # CORRECTED LINE: We are now iterating through a simple list of documents, not tuples.\n",
    "    context_text = \"\\n\\n\".join([\n",
    "        doc.page_content + \"\\nFile: \" + doc.metadata.get('source', 'Unknown')\n",
    "        for doc in docs_chroma  # Removed the \", _score\" part here\n",
    "    ])\n",
    "    \n",
    "    if past_messages != \"\":\n",
    "        context_text = \"Past questions and answers:\\n\\n\" + past_messages + \"\\n\\nNew Context for question:\\n\\n\" + context_text\n",
    "    \n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are a helpful data analyst for a company.\n",
    "    Your goal is to provide correct and accurate answers based on the context provided.\n",
    "    Provide direct concise answers.\n",
    "    Don't make up information.\n",
    "    Mention The source that you based your answer on.\n",
    "    Make sure your answer is in correct Markdown format.\n",
    "    If you are going to include LaTeX equations in you answers, **use $$..$$ instead of \\[...\\]** and make sure it is correct LaTeX.\n",
    "    Include the source of your information in the answer at the end of it.\n",
    "    \n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # load retrieved context and user query in the prompt template\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query)\n",
    "    \n",
    "    # call LLM model to generate the answer based on the given context and query\n",
    "    model = ChatGroq(model=\"moonshotai/kimi-k2-instruct\", api_key=GROQ_API_KEY, temperature=0.5)\n",
    "\n",
    "    # Use the .stream() method to get a streaming response\n",
    "    stream = model.stream(prompt)\n",
    "    \n",
    "    # Yield the content of each chunk from the stream\n",
    "    for chunk in stream:\n",
    "        yield chunk.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb679d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str, langs=[\"en\"]) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF using Surya OCR and returns it as a single string.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        langs (list): List of language codes for OCR (e.g., [\"en\"] for English).\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Load Surya models and processors\n",
    "    det_processor, det_model = load_det_processor(), load_det_model()\n",
    "    rec_model, rec_processor = load_rec_model(), load_rec_processor()\n",
    "\n",
    "    # Convert PDF pages to images\n",
    "    images = convert_from_path(pdf_path)\n",
    "\n",
    "    # Run OCR on each page and collect text\n",
    "    extracted_text = \"\"\n",
    "    for image in images:\n",
    "        \n",
    "        # Perform OCR\n",
    "        predictions = run_ocr([image], [langs], det_model, det_processor, rec_model, rec_processor)\n",
    "        \n",
    "        # Extract text lines from predictions\n",
    "        for page in predictions:\n",
    "            for line in page.text_lines:\n",
    "                extracted_text += line.text + \"\\n\"\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff4c02",
   "metadata": {},
   "source": [
    "# Function to process PDFs using glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab994d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_extracted_text_from_pdfs(pdf_path:str, output_dir=\"temp_results\", langs=[\"en\"]) -> str:\n",
    "    \"\"\"\n",
    "    Processes the PDF file at the given path and saves the extracted text to a text file.\n",
    "    If the text file already exists, it skips processing that PDF.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        output_dir (str): Directory to save the extracted text files.\n",
    "        langs (list): List of language codes for OCR (e.g., [\"en\"] for English\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the saved text file\n",
    "    \"\"\"\n",
    "    # Define the output text file path\n",
    "    text_file_path = os.path.join(output_dir, \"/\".join(os.path.splitext(pdf_path)[0].split('/')[1:]) + \".txt\")\n",
    "    print(text_file_path)\n",
    "\n",
    "    # check if the file path provided as an argument exists\n",
    "    if os.path.exists(text_file_path):\n",
    "        print(f\"Skipping {pdf_path} as output file already exists.\")\n",
    "        return text_file_path\n",
    "    \n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "\n",
    "    # Extract text from PDF\n",
    "    extracted_text = extract_text_from_pdf(pdf_path, langs=langs)\n",
    "\n",
    "    # Create output directory if it does not exist\n",
    "    os.makedirs(os.path.dirname(text_file_path), exist_ok=True)\n",
    "\n",
    "    # Save the extracted text\n",
    "    with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(extracted_text)\n",
    "\n",
    "    print(f\"Saved extracted text to: {text_file_path}\")\n",
    "    return text_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "261a65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_RAG(query:str, past_messages=[], chat_name=False, DEBUG=False) -> list:\n",
    "    \"\"\"\n",
    "    Processes a query against local documents to generate a streaming answer, preserving\n",
    "    all original functionality including chat history trimming and specific model versions.\n",
    "    Includes an optional DEBUG flag for detailed, commented logging.\n",
    "\n",
    "    Returns a list:\n",
    "    - On error: [1, \"error_message_string\"]\n",
    "    - On success: [0, <generator_object>] or [0, <generator_object>, \"chat_name\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"#-----------------------------------------#\")\n",
    "        print(\"#    1. Security Guard Check              #\")\n",
    "        print(\"#-----------------------------------------#\")\n",
    "        print(\"Checking query safety...\")\n",
    "    \n",
    "    try:\n",
    "        llm_guard = ChatGroq(model=\"meta-llama/llama-prompt-guard-2-86m\", api_key=GROQ_API_KEY)\n",
    "        guard_response = llm_guard.invoke(query)\n",
    "        score = float(guard_response.content)\n",
    "\n",
    "        # If the score is > 0.5, the query is considered unsafe.\n",
    "        if score > 0.5:\n",
    "            if DEBUG: print(f\"Query flagged as unsafe with score: {score}. Halting process.\")\n",
    "            # Return status code 1 for unsafe query\n",
    "            return [1, \"The query is considered unsafe and will not be processed.\"]\n",
    "        if DEBUG: print(\"Query is safe, proceeding with RAG pipeline.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        if DEBUG: print(f\"An error occurred during the safety check: {e}\")\n",
    "        return [1, \"Could not verify the safety of the query.\"]\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"\\n#-----------------------------------------#\")\n",
    "        print(\"#   2. Data Indexing and Retrieval        #\")\n",
    "        print(\"#-----------------------------------------#\")\n",
    "    if DEBUG: print(f\"Processing query: {query}\")\n",
    "\n",
    "    # --- Handling Chat History ---\n",
    "    if DEBUG: print(\"\\n# --- Handling Chat History ---\")\n",
    "    trimmed_messages = \"\"\n",
    "    if len(past_messages) > 0:\n",
    "        if DEBUG: print(f\"Found {len(past_messages)} past messages. Trimming for context...\")\n",
    "        messages = []\n",
    "        for message in past_messages:\n",
    "            for k, v in message.items():\n",
    "                if k == \"human\":\n",
    "                    messages.append(HumanMessage(v))\n",
    "                elif k == 'ai':\n",
    "                    messages.append(AIMessage(v))\n",
    "        \n",
    "        trimmed_messages = trim_messages(\n",
    "            messages,\n",
    "            strategy=\"last\",\n",
    "            token_counter=ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY),\n",
    "            max_tokens=5196,\n",
    "            start_on=\"human\",\n",
    "            end_on=(\"human\", \"tool\"),\n",
    "            include_system=True,\n",
    "            allow_partial=True,\n",
    "        )\n",
    "        trimmed_messages = \"\\n\".join([t.content for t in trimmed_messages])\n",
    "        if DEBUG: print(\"Chat history processed.\")\n",
    "    else:\n",
    "        if DEBUG: print(\"No past messages found.\")\n",
    "\n",
    "    # --- Finding and Loading Files ---\n",
    "    if DEBUG: print(\"\\n# --- Finding and Loading Files ---\")\n",
    "    Files = [f for f in glob.glob(\"Data/**/*\", recursive=True) if os.path.isfile(f)]\n",
    "    if DEBUG: print(f\"Found {len(Files)} files to process.\")\n",
    "    \n",
    "    docs = []\n",
    "    for file in Files:\n",
    "        if DEBUG: print(f\"  -> Loading file: {os.path.basename(file)}\")\n",
    "        if file.lower().endswith((\"xlsx\", \"xls\")):\n",
    "            docs_loaded = load_excel_files(file)\n",
    "            for doc in docs_loaded:\n",
    "                doc.metadata['filename'] = os.path.basename(file)\n",
    "            docs.extend(docs_loaded)\n",
    "        elif file.lower().endswith(\"txt\"):\n",
    "            docs_loaded = load_text_files(file)\n",
    "            for doc in docs_loaded:\n",
    "                doc.metadata['filename'] = os.path.basename(file)\n",
    "            docs.extend(docs_loaded)\n",
    "        elif file.lower().endswith(\"pdf\"):\n",
    "            text_path = save_extracted_text_from_pdfs(file)\n",
    "            text = load_text_files(text_path)\n",
    "            for doc in text:\n",
    "                doc.metadata['filename'] = os.path.basename(file)\n",
    "            docs.extend(text)\n",
    "        else:\n",
    "            if DEBUG: print(f\"Unsupported file type found: {file}\")\n",
    "            return [1, \"Unsupported file\"]\n",
    "    if DEBUG: print(\"Finished loading all documents.\")\n",
    "\n",
    "    # --- Splitting, Embedding, and Retrieving ---\n",
    "    if DEBUG: print(\"\\n# --- Splitting, Embedding, and Retrieving ---\")\n",
    "    chunks = split_documents(docs, chunk_size=2048, chunk_overlap=512)\n",
    "    if DEBUG: print(f\"Split documents into {len(chunks)} chunks.\")\n",
    "    \n",
    "    db_chroma = store_embeddings(chunks)\n",
    "    if DEBUG: print(\"Finished embedding chunks into vector store.\")\n",
    "    \n",
    "    docs_retrieved = retrieve_documents(db_chroma, chunks, query, k=20)\n",
    "    if DEBUG: print(f\"Retrieved {len(docs_retrieved)} relevant chunks for the query.\")\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"\\n#-----------------------------------------#\")\n",
    "        print(\"#   3. Generation and Final Return        #\")\n",
    "        print(\"#-----------------------------------------#\")\n",
    "        print(\"Data pipeline complete. Creating response generator...\")\n",
    "\n",
    "    # The streaming generator is created here\n",
    "    response_generator = generate_answer(docs_retrieved, query, trimmed_messages)\n",
    "\n",
    "    # --- Handle Chat Name Generation ---\n",
    "    if chat_name:\n",
    "        if DEBUG: print(\"Chat name requested. Generating with llama-3.1-8b-instant...\")\n",
    "        try:\n",
    "            llm = ChatGroq(model=\"llama-3.1-8b-instant\", api_key=os.environ['GROQ_API_KEY'])\n",
    "            name_prompt = f\"Give me a sentence as a name for this chat if the first question is \\\"{query}\\\". Return only the name and nothing else. Limit the name to 15 characters max. Make it readable and understandable.\"\n",
    "            answer = llm.invoke(name_prompt)\n",
    "            name_content = answer.content.strip()\n",
    "            return [0, response_generator, name_content]\n",
    "        except Exception as e:\n",
    "            if DEBUG: print(f\"Could not generate chat name: {e}. Proceeding without it.\")\n",
    "            # Fallback to returning two elements if name generation fails\n",
    "            return [0, response_generator]\n",
    "\n",
    "    # Return list with TWO elements on success without chat name\n",
    "    return [0, response_generator]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f479215",
   "metadata": {},
   "source": [
    "# Testing the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Notebook Test Cell (Human-Readable Version) ---\n",
    "\n",
    "# import time\n",
    "# import sys\n",
    "# from IPython.display import display, Markdown # For better rendering\n",
    "\n",
    "# # --- 1. Configuration ---\n",
    "# # Set the DEBUG flag here to turn diagnostic logs on or off.\n",
    "# # This makes it easy to switch between a quiet run and a detailed analysis.\n",
    "# IS_DEBUG_MODE = False\n",
    "\n",
    "# # --- 2. Execution ---\n",
    "# print(f\"Calling the RAG pipeline in {'DEBUG' if IS_DEBUG_MODE else 'NORMAL'} mode...\")\n",
    "# result_list = call_RAG(\n",
    "#     \"how much was the profit 2025?\", \n",
    "#     chat_name=True,\n",
    "#     DEBUG=IS_DEBUG_MODE\n",
    "# )\n",
    "# print(\"\\n...Pipeline call has returned.\")\n",
    "\n",
    "# # --- 3. Process the Result ---\n",
    "# status_flag = result_list[0]\n",
    "# print(\"-\" * 40)\n",
    "\n",
    "# if status_flag == 0:\n",
    "#     print(\"STATUS: Success. Now processing the response stream...\\n\")\n",
    "    \n",
    "#     # The second element is the generator object. We need to loop through it.\n",
    "#     response_generator = result_list[1]\n",
    "    \n",
    "#     # We use sys.stdout.flush and a small sleep to counteract Jupyter's\n",
    "#     # output buffering, ensuring we see the text as it arrives.\n",
    "#     full_response_text = \"\"\n",
    "#     for chunk in response_generator:\n",
    "#         full_response_text += chunk\n",
    "#         sys.stdout.write(chunk)\n",
    "#         sys.stdout.flush()\n",
    "#         time.sleep(0.005) # A very small delay is enough.\n",
    "        \n",
    "#     print(\"\\n\\n\" + \"-\"*40)\n",
    "#     print(\"STREAM COMPLETE.\")\n",
    "\n",
    "#     # Check if a chat name was also returned.\n",
    "#     if len(result_list) > 2:\n",
    "#         chat_name = result_list[2]\n",
    "#         print(f\"Generated Chat Name: '{chat_name}'\")\n",
    "    \n",
    "#     # For a nice final output, we can render the full markdown response.\n",
    "#     # display(Markdown(full_response_text))\n",
    "\n",
    "# else: # status_flag is 1\n",
    "#     # On error, the second element is the error message string.\n",
    "#     error_message = result_list[1]\n",
    "#     print(f\"STATUS: Error\\nMESSAGE: {error_message}\")\n",
    "\n",
    "# print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44271b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_z",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
