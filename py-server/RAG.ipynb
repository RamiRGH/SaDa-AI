{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e2d958",
   "metadata": {},
   "source": [
    "# Needed Packages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26283d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install uv\n",
    "# %uv pip install --upgrade langchain langchain-community langchain-chroma\n",
    "# %uv pip install -qU langchain-groq\n",
    "# %uv pip install langchain_openai\n",
    "# %uv pip install --upgrade langchain_huggingface\n",
    "# %uv pip install --upgrade unstructured openpyxl\n",
    "# %uv pip install nltk\n",
    "# %uv pip install --upgrade --quiet langchain sentence_transformers\n",
    "# %uv pip install xlrd\n",
    "# %uv pip install xformers\n",
    "# %uv pip install pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45e363",
   "metadata": {},
   "source": [
    "* NOTE:\n",
    "    The `embedding model (Jina Embeddings V3)` and the `LLM (Llama 3.2 90b)` are used through API services offered by Jina AI and Groq respictively.\n",
    "    However, both are open-source and can be downloaded and used locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47910163",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c99dfa",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71358248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import nltk\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import glob\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_core.messages import trim_messages, AIMessage, HumanMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "## Imports for the PDF to text conversion using surya-ocr\n",
    "from PIL import Image\n",
    "# from surya.ocr import run_ocr\n",
    "# from surya.model.detection.model import load_model as load_det_model, load_processor as load_det_processor\n",
    "# from surya.model.recognition.model import load_model as load_rec_model\n",
    "# from surya.model.recognition.processor import load_processor as load_rec_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39f6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install uv\n",
    "\n",
    "# # Core LangChain\n",
    "# ! uv pip install langchain\n",
    "\n",
    "# # For Ollama integration (embeddings, models)\n",
    "# ! uv pip install langchain-ollama\n",
    "\n",
    "# # For Chroma vector database\n",
    "# ! uv pip install langchain-chroma chromadb\n",
    "\n",
    "# # For community loaders (Arxiv, PDF, text, etc.)\n",
    "# ! uv pip install langchain-community\n",
    "\n",
    "# # For text splitting utilities\n",
    "# ! uv pip install langchain-text-splitters\n",
    "\n",
    "# # Core types, prompts, and documents (usually installed with langchain-core)\n",
    "# ! uv pip install langchain-core\n",
    "\n",
    "# # Typing extensions (for TypedDict, List, etc.)\n",
    "# ! uv pip install typing-extensions\n",
    "\n",
    "# # LangGraph (for StateGraph)@\n",
    "# ! uv pip install langgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb133c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import ArxivLoader, TextLoader\n",
    "from langchain_community.document_loaders.pdf import BasePDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing_extensions import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langgraph.graph import StateGraph, START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c954df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = init_chat_model(model=\"gemma3:1b\", model_provider=\"ollama\")\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the SQLite database for LLM caching\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# # API keys\n",
    "GROQ_API_KEY = os.environ['GROQ_API_KEY']\n",
    "# JINA_API_KEY = os.environ['JINA_API_KEY']\n",
    "\n",
    "# Needed downloads for nltk (Only needs to be done once)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2a48e",
   "metadata": {},
   "source": [
    "# Data Indexing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd0c17",
   "metadata": {},
   "source": [
    "## 1- Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94142b3d",
   "metadata": {},
   "source": [
    "### For Excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24976c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_files(file_paths:list) -> list:\n",
    "    \"\"\"\n",
    "    Load Excel files and return a list of Langchain Documents.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list): List of file paths to Excel files\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Langchain Documents\n",
    "    \"\"\"\n",
    "    loader = UnstructuredExcelLoader(file_paths, mode='elements')\n",
    "    doc = loader.load()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa5460",
   "metadata": {},
   "source": [
    "### For txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6845da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_files(file_path:str):\n",
    "    \"\"\"\n",
    "    Loads a text file and returns a Langchain Document.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the text file\n",
    "\n",
    "    Returns:\n",
    "        Document: A Langchain Document\n",
    "    \"\"\"\n",
    "    loader = TextLoader(file_path=file_path, encoding='utf-8')\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8999e13",
   "metadata": {},
   "source": [
    "## 2-Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f871304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the doc into smaller chunks i.e. chunk_size=512\n",
    "def split_documents(docs: list, chunk_size=512, chunk_overlap=128) -> list:\n",
    "    \"\"\"\n",
    "    Splits the provided documents into smaller chunks with specified size and overlap.\n",
    "\n",
    "    Parameters:\n",
    "        docs (list): List of documents to be split.\n",
    "        chunk_size (int, optional): The number of characters in each chunk (default is 512).\n",
    "        chunk_overlap (int, optional): The number of overlapping characters between chunks (default is 128).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of split document chunks with corrected metadata.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Fixing the metadata if something is wrong with it\n",
    "    for chunk in chunks:\n",
    "        for key, value in chunk.metadata.items():\n",
    "            if isinstance(value, list):\n",
    "                chunk.metadata[key] = ','.join(value)  # Convert list to a comma-separated string\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c6be3",
   "metadata": {},
   "source": [
    "## 3-Data Embedding and data storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(chunks: list, CHROMA_PATH=\"vec_db\"):\n",
    "    \"\"\"\n",
    "    Store the given chunks as embeddings in a Chroma database.\n",
    "\n",
    "    Parameters:\n",
    "        chunks (list of Document): list of documents to be embedded\n",
    "        CHROMA_PATH (str, optional): path where the Chroma database is stored (default is \"vec_db\")\n",
    "\n",
    "    Returns:\n",
    "        Chroma: the created Chroma database\n",
    "    \"\"\"\n",
    "    store = LocalFileStore(\"./emb_cache/\")\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "    cached_embedder = CacheBackedEmbeddings.from_bytes_store(embeddings, store, query_embedding_cache=True)\n",
    "    \n",
    "    # embed the chunks as vectors and load them into the database\n",
    "    db_chroma = Chroma.from_documents(chunks, cached_embedder, persist_directory=CHROMA_PATH)\n",
    "    return db_chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22fdcb",
   "metadata": {},
   "source": [
    "# Data Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efb9fe",
   "metadata": {},
   "source": [
    "## 1-Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff98df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(db_chroma, chunks, query: str, k=50) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Retrieve context - top k most relevant (closest) chunks to the query vector using an ensemble retriever.\n",
    "    \n",
    "    Parameters:\n",
    "        db_chroma (Chroma): database of embeddings\n",
    "        chunks (list): List of document chunks.\n",
    "        query (str or np.ndarray): user query as a string or a vector\n",
    "        k (int, optional): number of documents to retrieve (default is 50)\n",
    "        \n",
    "    Returns:\n",
    "        list of tuple: list of retrieved documents and their scores\n",
    "    \"\"\"\n",
    "    # Initialize the BM25 retriever\n",
    "    bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "    bm25_retriever.k = k\n",
    "\n",
    "    # Initialize the Chroma retriever\n",
    "    chroma_retriever = db_chroma.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    # Initialize the Ensemble Retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, chroma_retriever], weights=[0.6, 0.4]\n",
    "    )\n",
    "    \n",
    "    docs = ensemble_retriever.invoke(query)\n",
    "    # The ensemble retriever in langchain doesn't support returning scores, so we return a list of documents\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b8b5d",
   "metadata": {},
   "source": [
    "## 2-Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d164a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(docs_chroma: list, query: str, past_messages:str) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer based on given user query and retrieved context information\n",
    "    \n",
    "    Parameters:\n",
    "        docs_chroma (list of Document): retrieved context information\n",
    "        query (str): user query\n",
    "        past_messages (str): past questions and answers\n",
    "    Returns:\n",
    "        str: answer to the user query\n",
    "    \"\"\"\n",
    "\n",
    "    # CORRECTED LINE: We are now iterating through a simple list of documents, not tuples.\n",
    "    context_text = \"\\n\\n\".join([\n",
    "        doc.page_content + \"\\nFile: \" + doc.metadata.get('source', 'Unknown')\n",
    "        for doc in docs_chroma  # Removed the \", _score\" part here\n",
    "    ])\n",
    "    \n",
    "    if past_messages != \"\":\n",
    "        context_text = \"Past questions and answers:\\n\\n\" + past_messages + \"\\n\\nNew Context for question:\\n\\n\" + context_text\n",
    "    \n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are a helpful data analyst for a company.\n",
    "    Your goal is to provide correct and accurate answers based on the context provided.\n",
    "    Provide direct concise answers.\n",
    "    Don't make up information.\n",
    "    Mention The source that you based your answer on.\n",
    "    Make sure your answer is in correct Markdown format.\n",
    "    If you are going to include LaTeX equations in you answers, **use $$..$$ instead of \\[...\\]** and make sure it is correct LaTeX.\n",
    "    \n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # load retrieved context and user query in the prompt template\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query)\n",
    "    \n",
    "    # call LLM model to generate the answer based on the given context and query\n",
    "    model = ChatGroq(model=\"compound-beta\", api_key=GROQ_API_KEY)\n",
    "\n",
    "    # Use the .stream() method to get a streaming response\n",
    "    stream = model.stream(prompt)\n",
    "    \n",
    "    # Yield the content of each chunk from the stream\n",
    "    for chunk in stream:\n",
    "        yield chunk.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb679d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str, langs=[\"en\"]) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF using Surya OCR and returns it as a single string.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        langs (list): List of language codes for OCR (e.g., [\"en\"] for English).\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Load Surya models and processors\n",
    "    det_processor, det_model = load_det_processor(), load_det_model()\n",
    "    rec_model, rec_processor = load_rec_model(), load_rec_processor()\n",
    "\n",
    "    # Convert PDF pages to images\n",
    "    images = convert_from_path(pdf_path)\n",
    "\n",
    "    # Run OCR on each page and collect text\n",
    "    extracted_text = \"\"\n",
    "    for image in images:\n",
    "        \n",
    "        # Perform OCR\n",
    "        predictions = run_ocr([image], [langs], det_model, det_processor, rec_model, rec_processor)\n",
    "        \n",
    "        # Extract text lines from predictions\n",
    "        for page in predictions:\n",
    "            for line in page.text_lines:\n",
    "                extracted_text += line.text + \"\\n\"\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff4c02",
   "metadata": {},
   "source": [
    "# Function to process PDFs using glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab994d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_extracted_text_from_pdfs(pdf_path:str, output_dir=\"temp_results\", langs=[\"en\"]) -> str:\n",
    "    \"\"\"\n",
    "    Processes the PDF file at the given path and saves the extracted text to a text file.\n",
    "    If the text file already exists, it skips processing that PDF.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        output_dir (str): Directory to save the extracted text files.\n",
    "        langs (list): List of language codes for OCR (e.g., [\"en\"] for English\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the saved text file\n",
    "    \"\"\"\n",
    "    # Define the output text file path\n",
    "    text_file_path = os.path.join(output_dir, \"/\".join(os.path.splitext(pdf_path)[0].split('/')[1:]) + \".txt\")\n",
    "    print(text_file_path)\n",
    "\n",
    "    # check if the file path provided as an argument exists\n",
    "    if os.path.exists(text_file_path):\n",
    "        print(f\"Skipping {pdf_path} as output file already exists.\")\n",
    "        return text_file_path\n",
    "    \n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "\n",
    "    # Extract text from PDF\n",
    "    extracted_text = extract_text_from_pdf(pdf_path, langs=langs)\n",
    "\n",
    "    # Create output directory if it does not exist\n",
    "    os.makedirs(os.path.dirname(text_file_path), exist_ok=True)\n",
    "\n",
    "    # Save the extracted text\n",
    "    with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(extracted_text)\n",
    "\n",
    "    print(f\"Saved extracted text to: {text_file_path}\")\n",
    "    return text_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_RAG(query:str, past_messages=[], chat_name=False) -> list:\n",
    "    \"\"\"\n",
    "    Processes a query and past messages, retrieves relevant documents, and generates an answer.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The user query to process and answer.\n",
    "        past_messages (list): A list of past conversation messages, each being a dictionary with \n",
    "            keys \"human\" and \"ai\".\n",
    "        chat_name (bool): Flag indicating whether to generate a chat name based on the query.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing a status code and response content. If chat_name is True, \n",
    "        it also includes a generated name for the chat.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. NEW: Security Guard Check ---\n",
    "    # This is now the first step in the pipeline.\n",
    "    try:\n",
    "        print(\"Checking query safety...\")\n",
    "        llm_guard = ChatGroq(model=\"meta-llama/llama-prompt-guard-2-86m\", api_key=GROQ_API_KEY)\n",
    "        guard_response = llm_guard.invoke(query)\n",
    "        score = float(guard_response.content)\n",
    "\n",
    "        # If the score is > 0.5, the query is considered unsafe.\n",
    "        if score > 0.5:\n",
    "            print(f\"Query flagged as unsafe with score: {score}. Halting process.\")\n",
    "            # Return status code 1 for unsafe query\n",
    "            return [1, \"The query is considered unsafe and will not be processed.\"]\n",
    "            \n",
    "        print(\"Query is safe, proceeding with RAG pipeline.\")\n",
    "    except Exception as e:\n",
    "        # Handle cases where the guard model might fail or return unexpected content\n",
    "        print(f\"An error occurred during the safety check: {e}\")\n",
    "        return [1, \"Could not verify the safety of the query.\"]\n",
    "\n",
    "    # --- 2. Existing RAG Pipeline ---\n",
    "    # This code only runs if the query passes the safety check.\n",
    "    \n",
    "    print(f\"Processing query: {query}\")\n",
    "    \n",
    "    # Get all files in Data and all nested subfolders\n",
    "    Files = [f for f in glob.glob(\"Data/**/*\", recursive=True) if os.path.isfile(f)]\n",
    "    \n",
    "    trimmed_messages = \"\"\n",
    "    \n",
    "    if len(past_messages) > 0:\n",
    "        messages = []\n",
    "        for message in past_messages:\n",
    "            for k, v in message.items():\n",
    "                if k == \"human\":\n",
    "                    messages.append(HumanMessage(v))\n",
    "                elif k == 'ai':\n",
    "                    messages.append(AIMessage(v))\n",
    "                    \n",
    "        trimmed_messages = trim_messages(\n",
    "            messages,\n",
    "            strategy=\"last\",\n",
    "            token_counter=ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=GROQ_API_KEY),\n",
    "            max_tokens=5196,\n",
    "            start_on=\"human\",\n",
    "            end_on=(\"human\", \"tool\"),\n",
    "            include_system=True,\n",
    "            allow_partial=True,\n",
    "        )    \n",
    "\n",
    "        trimmed_messages = \"\\n\".join([t.content for t in trimmed_messages])\n",
    "        \n",
    "    # Load files \n",
    "    docs = []\n",
    "    for file in Files:\n",
    "        if file.lower().endswith((\"xlsx\", \"xls\")):\n",
    "            docs_loaded = load_excel_files(file)\n",
    "            for doc in docs_loaded:\n",
    "                doc.metadata['filename'] = os.path.basename(file)\n",
    "            docs.extend(docs_loaded)\n",
    "    \n",
    "        elif file.lower().endswith(\"txt\"):\n",
    "            docs_loaded = load_text_files(file)\n",
    "            for doc in docs_loaded:\n",
    "                doc.metadata['filename'] = os.path.basename(file)\n",
    "            docs.extend(docs_loaded)\n",
    "    \n",
    "        elif file.lower().endswith(\"pdf\"):\n",
    "            text_path = save_extracted_text_from_pdfs(file)\n",
    "            text = load_text_files(text_path)\n",
    "            for doc in text:\n",
    "                doc.metadata['filename'] = os.path.basename(file)\n",
    "            docs.extend(text)\n",
    "    \n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file}. Supported types are: .xlsx, .xls, .txt, .pdf\")\n",
    "            return [1, \"Unsupported file\"]\n",
    "\n",
    "    # Split and store documents\n",
    "    chunks = split_documents(docs, chunk_size=2048, chunk_overlap=512)\n",
    "    db_chroma = store_embeddings(chunks)\n",
    "\n",
    "    # Use the ensemble retriever for hybrid search\n",
    "    docs_retrieved = retrieve_documents(db_chroma, chunks, query, k=20)\n",
    "    # for num, doc in enumerate(docs_retrieved):\n",
    "    #     print(f\"\\n\\nChunk {num}: \\n{doc}\")\n",
    "    # print(\"\\n\\n\\n\")\n",
    "    response_generator = generate_answer(docs_retrieved, query, trimmed_messages)\n",
    "    \n",
    "    if chat_name:\n",
    "        try:\n",
    "            print(\"Generating chat name...\")\n",
    "            llm = ChatGroq(model=\"llama-3.1-8b-instant\", api_key=os.environ['GROQ_API_KEY'])\n",
    "            name_prompt = f\"Give me a sentence as a name for this chat if the first question is \\\"{query}\\\". Return only the name and nothing else. Limit the name to 15 characters max. Make it readable and understandable.\"\n",
    "            answer = llm.invoke(name_prompt)\n",
    "            name_content = answer.content.strip()\n",
    "            # Return list with THREE elements on success with chat name\n",
    "            return [0, response_generator, name_content]\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate chat name: {e}. Proceeding without it.\")\n",
    "            # Fallback to returning two elements if name generation fails\n",
    "            return [0, response_generator]\n",
    "\n",
    "    # Return list with TWO elements on success without chat name\n",
    "    return [0, response_generator]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f479215",
   "metadata": {},
   "source": [
    "# Testing the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf1d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, '### ELM Company Profit Calculation\\nTo calculate the total profit for ELM in 2022 and 2023, we need to find the relevant information in the provided context.\\n\\n#### Profit for 2023\\nThe profit for the nine months period ended September 30, 2023, is mentioned in the file `Q3.txt`:\\n- NET PROFIT for the nine months period ended September 30, 2023: 1,029,419,971 Saudi Riyals\\n\\n#### Profit for 2024 and other years\\nThere is no information available about the profit for the full year 2022 or 2023. However, the profit for the nine months period ended September 30, 2022, is not directly available, but we have information for 2024:\\n- NET PROFIT for the three months period ended September 30, 2024: 498,241,621 Saudi Riyals\\n- NET PROFIT for the nine months period ended September 30, 2024: 1,329,249,673 Saudi Riyals\\n\\n#### Profit for 2022\\nWe do not have direct information about the profit for the full year 2022.\\n\\n#### Total Profit Calculation\\nSince we do not have the profit information for the full year 2022 and 2023, we cannot calculate the total profit accurately. However, we can calculate the total profit for the available periods:\\n- Total profit for the nine months period ended September 30, 2023, and the nine months period ended September 30, 2024: \\n  - 1,029,419,971 (2023) + 1,329,249,673 (2024) = 2,358,669,644 Saudi Riyals\\n\\nPlease note that this calculation is based on the available information and may not represent the total profit for the full years 2022 and 2023.\\n\\nThe information was found in the following files:\\n* `Q3.txt` \\n\\nThis answer is based only on the provided context and may not reflect the actual financial situation of ELM Company.']\n"
     ]
    }
   ],
   "source": [
    "# lst = call_RAG(\"how much was the profit for ELM in 2022 and 2023? add them together and return the total profit in a single number\")\n",
    "# print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ELM Company Profit Calculation\n",
      "To calculate the total profit for ELM in 2022 and 2023, we need to find the relevant information in the provided context.\n",
      "\n",
      "#### Profit for 2023\n",
      "The profit for the nine months period ended September 30, 2023, is mentioned in the file `Q3.txt`:\n",
      "- NET PROFIT for the nine months period ended September 30, 2023: 1,029,419,971 Saudi Riyals\n",
      "\n",
      "#### Profit for 2024 and other years\n",
      "There is no information available about the profit for the full year 2022 or 2023. However, the profit for the nine months period ended September 30, 2022, is not directly available, but we have information for 2024:\n",
      "- NET PROFIT for the three months period ended September 30, 2024: 498,241,621 Saudi Riyals\n",
      "- NET PROFIT for the nine months period ended September 30, 2024: 1,329,249,673 Saudi Riyals\n",
      "\n",
      "#### Profit for 2022\n",
      "We do not have direct information about the profit for the full year 2022.\n",
      "\n",
      "#### Total Profit Calculation\n",
      "Since we do not have the profit information for the full year 2022 and 2023, we cannot calculate the total profit accurately. However, we can calculate the total profit for the available periods:\n",
      "- Total profit for the nine months period ended September 30, 2023, and the nine months period ended September 30, 2024: \n",
      "  - 1,029,419,971 (2023) + 1,329,249,673 (2024) = 2,358,669,644 Saudi Riyals\n",
      "\n",
      "Please note that this calculation is based on the available information and may not represent the total profit for the full years 2022 and 2023.\n",
      "\n",
      "The information was found in the following files:\n",
      "* `Q3.txt` \n",
      "\n",
      "This answer is based only on the provided context and may not reflect the actual financial situation of ELM Company.\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook Test Cell ---\n",
    "\n",
    "# 1. Your original call to the function remains the same.\n",
    "#    Let's also ask for a chat name to test the full logic.\n",
    "print(\"--- Calling RAG Pipeline ---\")\n",
    "result_list = call_RAG(\n",
    "    \"how much was the profit for ELM in 2022 and 2023? add them together and return the total profit in a single number\", \n",
    "    chat_name=True\n",
    ")\n",
    "print(\"--- RAG Pipeline Call Finished ---\\n\")\n",
    "\n",
    "\n",
    "# 2. Check the status flag from the returned list\n",
    "status_flag = result_list[0]\n",
    "\n",
    "# 3. Process the result based on the flag\n",
    "if status_flag == 0:\n",
    "    print(\"--- Streaming Response (Status: Success) ---\")\n",
    "    \n",
    "    # On success, the second element (index 1) is the GENERATOR object.\n",
    "    response_generator = result_list[1]\n",
    "    \n",
    "    # You must loop through the generator to get the content chunks.\n",
    "    # The `end=\"\"` and `flush=True` arguments to print() will make the\n",
    "    # output appear as a continuous stream in your notebook cell.\n",
    "    for chunk in response_generator:\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        \n",
    "    # Print a final newline for clean formatting after the stream is complete.\n",
    "    print(\"\\n\" + \"-\"*41)\n",
    "    \n",
    "    # You can also check if a chat name was returned as the third element.\n",
    "    if len(result_list) > 2:\n",
    "        chat_name = result_list[2]\n",
    "        print(f\"Chat Name Generated: {chat_name}\")\n",
    "        print(\"-\"*41)\n",
    "\n",
    "else: # This block handles the case where status_flag is 1\n",
    "    print(\"--- Error Occurred ---\")\n",
    "    \n",
    "    # On error, the second element (index 1) is the error message STRING.\n",
    "    error_message = result_list[1]\n",
    "    print(f\"Error Message: {error_message}\")\n",
    "    print(\"-\"*22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
